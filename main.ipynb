{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento para el conjunto de datos Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'preprocess_data' from 'Neural_Network.utils' (c:\\Users\\leojs\\OneDrive - Universidad de Las Palmas de Gran Canaria\\Documents\\UNI\\OH\\ModularNNEngine\\Neural_Network\\utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mNeural_Network\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneural_network\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NeuralNetwork\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mNeural_Network\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split, preprocess_data\n\u001b[0;32m      6\u001b[0m X, y \u001b[38;5;241m=\u001b[39m load_iris_data() \n\u001b[0;32m      7\u001b[0m X, y \u001b[38;5;241m=\u001b[39m preprocess_data(X, y\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'preprocess_data' from 'Neural_Network.utils' (c:\\Users\\leojs\\OneDrive - Universidad de Las Palmas de Gran Canaria\\Documents\\UNI\\OH\\ModularNNEngine\\Neural_Network\\utils.py)"
     ]
    }
   ],
   "source": [
    "from data.load_data import load_iris_data\n",
    "import numpy as np\n",
    "from Neural_Network.neural_network import NeuralNetwork\n",
    "from Neural_Network.utils import train_test_split, preprocess_data\n",
    "\n",
    "X, y = load_iris_data() \n",
    "X, y = preprocess_data(X, y.reshape(-1, 1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_seed=42)\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "nn.train(X_train, y_train, learning_rate=0.2, epochs=750)\n",
    "\n",
    "y_pred_test = nn.forward(X_test)\n",
    "y_pred_classes_test = np.argmax(y_pred_test, axis=1)\n",
    "y_true_classes_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy_test = np.mean(y_pred_classes_test == y_true_classes_test)\n",
    "print(f\"Precisión en el conjunto de prueba: {accuracy_test * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento para el conjunto de datos MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\nfrom data.load_data import load_mnist_data\\nfrom Neural_Network.preprocess import preprocess_datamnist\\n\\nX, y = load_mnist_data() \\nX, y = preprocess_datamnist(X, y.reshape(-1, 1))\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_seed=42)\\n\\nnn = NeuralNetwork()\\nnn.train(X_train, y_train, learning_rate=0.2, epochs=750)\\n\\ny_pred_test = nn.forward(X_test)\\ny_pred_classes_test = np.argmax(y_pred_test, axis=1)\\ny_true_classes_test = np.argmax(y_test, axis=1)\\n\\naccuracy_test = np.mean(y_pred_classes_test == y_true_classes_test)\\nprint(f\"Precisión en el conjunto de prueba: {accuracy_test * 100:.2f}%\")\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "from data.load_data import load_mnist_data\n",
    "from Neural_Network.preprocess import preprocess_datamnist\n",
    "\n",
    "X, y = load_mnist_data() \n",
    "X, y = preprocess_datamnist(X, y.reshape(-1, 1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_seed=42)\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "nn.train(X_train, y_train, learning_rate=0.2, epochs=750)\n",
    "\n",
    "y_pred_test = nn.forward(X_test)\n",
    "y_pred_classes_test = np.argmax(y_pred_test, axis=1)\n",
    "y_true_classes_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy_test = np.mean(y_pred_classes_test == y_true_classes_test)\n",
    "print(f\"Precisión en el conjunto de prueba: {accuracy_test * 100:.2f}%\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
